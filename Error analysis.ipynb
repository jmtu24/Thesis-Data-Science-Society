{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this notebook was run on an external GPU in a Python file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from transformers import BertTokenizerFast as BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link to the dataset used for this thesis\n",
    "# https://www.kaggle.com/datasets/dkapitan/dutch-restaurant-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1856ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and transfer it to a dataframe\n",
    "file_path = file_path # Add the path to the file\n",
    "df_raw_data = pd.read_parquet(file_path) # load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062735da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of the raw dataset\n",
    "print(df_raw_data.head(5)) # Print the first five rows of the data\n",
    "print(len(df_raw_data)) # Print the number of rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dac326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary preprocessing steps for thesis\n",
    "df_pp_data = df_raw_data.copy()\n",
    "\n",
    "# Remove the unknown characters in the avgPrice column\n",
    "df_pp_data[\"avgPrice\"] = df_pp_data[\"avgPrice\"].str.replace(\"â\\u0082¬\",\"\")\n",
    "df_pp_data[\"avgPrice\"] = df_pp_data[\"avgPrice\"].str.replace(\"\\u0080\",\"\")\n",
    "\n",
    "# Convert the necessary numeric columns to numeric\n",
    "columns_to_convert = [\"scoreTotal\", \"avgPrice\", \"reviewerNumReviews\", \"reviewScoreOverall\", \"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\"]\n",
    "df_pp_data[columns_to_convert] = df_pp_data[columns_to_convert].apply(pd.to_numeric, errors = \"coerce\")\n",
    "\n",
    "# Remove reviews that are not positive and not negative\n",
    "df_pp_data = df_pp_data[~((df_pp_data[\"reviewScoreFood\"] == 5) | (df_pp_data[\"reviewScoreFood\"] == 6) |\n",
    "                         (df_pp_data[\"reviewScoreService\"] == 5) | (df_pp_data[\"reviewScoreService\"] == 6) |\n",
    "                           (df_pp_data[\"reviewScoreAmbiance\"] == 5) | (df_pp_data[\"reviewScoreAmbiance\"] == 6))]\n",
    "\n",
    "# change the personal review scores to sentiments\n",
    "columns_to_change = [\"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\"]\n",
    "bins = [1,6,10]\n",
    "labels = [\"Negative\", \"Positive\"]\n",
    "for column in columns_to_change:\n",
    "    df_pp_data[column] = pd.cut(df_pp_data[column], bins = bins, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the personal review scores to sentiments\n",
    "columns_to_change = [\"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\"]\n",
    "bins = [1,6,10]\n",
    "labels = [\"Negative\", \"Positive\"]\n",
    "for column in columns_to_change:\n",
    "    df_pp_data[column] = pd.cut(df_pp_data[column], bins = bins, labels = labels)\n",
    "\n",
    "# Extract the city name from the address\n",
    "def extract_second_to_last_element(address): # Function to extract the second to last element from the address\n",
    "    address_components = address.split()\n",
    "    if len(address_components) >= 2:\n",
    "        return address_components[-2]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_pp_data[\"City\"] = df_pp_data[\"address\"].apply(extract_second_to_last_element) # create the new feature\n",
    "\n",
    "unique_values_city = df_pp_data[\"City\"].value_counts() # Store the number of unique cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ac405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that contain missing values for the important variables\n",
    "# important variables: restoId, avgPrice, reviewScoreOverall, reviewScoreFood, reviewScoreService, reviewScoreAmbiance, reviewText, City\n",
    "df_nomissing = df_pp_data.copy()\n",
    "\n",
    "important_features = [\"restoId\", \"avgPrice\", \"reviewScoreOverall\", \"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\", \"reviewText\", \"City\"]\n",
    "\n",
    "df_nomissing = df_nomissing.dropna(subset = important_features)\n",
    "\n",
    "# Remove reviews that are too short and too long\n",
    "df_nomissing[\"reviewLength\"] = df_nomissing[\"reviewText\"].apply(len) # make a new feature that contains the length of a review\n",
    "\n",
    "descriptives_length = df_nomissing[\"reviewLength\"].describe()\n",
    "\n",
    "df_lessreviews = df_nomissing.copy()\n",
    "df_lessreviews = df_lessreviews[(df_lessreviews[\"reviewLength\"] >= 20) & (df_lessreviews[\"reviewLength\"] <= 2000)] # Reviews with less than 20 characters and more than 2000 characters will be removed\n",
    "\n",
    "# Randomly remove rows with all positive labels to make the dataset more balanced (is still imbalanced after, but less imbalanced),\n",
    "# also remove rows because the amount of data is too computationally expensive\n",
    "\n",
    "sentiment_columns = [\"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\"]\n",
    "df_sentiment = df_lessreviews[sentiment_columns]\n",
    "\n",
    "positive_rows = np.all(df_sentiment == \"Positive\", axis = 1) # Define which reviews only contain positive sentiments\n",
    "\n",
    "sampled_positive_rows = df_lessreviews[positive_rows].sample(n = 232830, random_state = 68)\n",
    "\n",
    "df_lessreviews = df_lessreviews.drop(sampled_positive_rows.index)\n",
    "\n",
    "# analyze how many positive and negative reviews are left in the reduced dataframe\n",
    "counts_food = df_lessreviews[\"reviewScoreFood\"].value_counts()\n",
    "counts_service = df_lessreviews[\"reviewScoreService\"].value_counts()\n",
    "counts_ambiance = df_lessreviews[\"reviewScoreAmbiance\"].value_counts()\n",
    "\n",
    "print(counts_food)\n",
    "print(counts_service)\n",
    "print(counts_ambiance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentiments into labels (negative = 0, positive = 1)\n",
    "class_mapping = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "# Initialize LabelEncoder with the custom mapping\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = class_mapping.keys()\n",
    "label_encoder.transform = lambda x: [class_mapping[label] for label in x]\n",
    "\n",
    "# Fit and transform the target variables using the custom mapping\n",
    "df_lessreviews[\"labelFood\"] = label_encoder.transform(df_lessreviews[\"reviewScoreFood\"])\n",
    "df_lessreviews[\"labelService\"] = label_encoder.transform(df_lessreviews[\"reviewScoreService\"])\n",
    "df_lessreviews[\"labelAmbiance\"] = label_encoder.transform(df_lessreviews[\"reviewScoreAmbiance\"])\n",
    "df_lessreviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db463f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a train, validation, and test set\n",
    "df_final = df_lessreviews\n",
    "\n",
    "# First split the data into train and temp sets\n",
    "df_train, df_temp = train_test_split(df_final, test_size= 0.2, random_state=68)\n",
    "\n",
    "# Then split the temp set into validation and test sets\n",
    "df_validation, df_test = train_test_split(df_temp, test_size= 0.5, random_state=68)\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print(\"Training set samples:\", len(df_train))\n",
    "print(\"Validation set samples:\", len(df_validation))\n",
    "print(\"Test set samples:\", len(df_test))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data for BERTje\n",
    "tokenizer_BERTje = BertTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\") # load the tokenizer for BERTje\n",
    "\n",
    "# The following code for tokenizing is based on the function wrap_examples(examples, tokenizer) available here: https://github.com/wietsedv/bertje/blob/master/finetuning/v1/run_110kDBRD.py\n",
    "# I used parts of this function to tokenize the reviews and return them as a dataset for my train, validation, and test samples seperately.\n",
    "\n",
    "# Tokenize the reviews in the training set\n",
    "train_input_ids, train_input_masks, train_labels = [], [], []\n",
    "for text in df_train[\"reviewText\"]:\n",
    "    tokenized_bert_train = tokenizer_BERTje.encode_plus(text, max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    train_input_ids.append(tokenized_bert_train[\"input_ids\"])\n",
    "    train_input_masks.append(tokenized_bert_train[\"attention_mask\"])\n",
    "\n",
    "train_input_ids = torch.tensor(train_input_ids, dtype = torch.long)\n",
    "train_input_masks = torch.tensor(train_input_masks, dtype = torch.long)\n",
    "train_labels = torch.tensor(df_train[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values, dtype = torch.float32)\n",
    "\n",
    "# Tokenize the reviews in the validation set\n",
    "validation_input_ids, validation_input_masks, validation_labels = [], [], []\n",
    "for text in df_validation[\"reviewText\"]:\n",
    "    tokenized_bert_validation = tokenizer_BERTje.encode_plus(text, max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    validation_input_ids.append(tokenized_bert_validation[\"input_ids\"])\n",
    "    validation_input_masks.append(tokenized_bert_validation[\"attention_mask\"])\n",
    "\n",
    "validation_input_ids = torch.tensor(validation_input_ids, dtype = torch.long)\n",
    "validation_input_masks = torch.tensor(validation_input_masks, dtype = torch.long)\n",
    "validation_labels = torch.tensor(df_validation[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values, dtype = torch.float32)\n",
    "\n",
    "# Tokenize the reviews in the test set\n",
    "test_input_ids, test_input_masks, test_labels = [], [], []\n",
    "for text in df_test[\"reviewText\"]:\n",
    "    tokenized_bert_test = tokenizer_BERTje.encode_plus(text, max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    test_input_ids.append(tokenized_bert_test[\"input_ids\"])\n",
    "    test_input_masks.append(tokenized_bert_test[\"attention_mask\"])\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids, dtype = torch.long)\n",
    "test_input_masks = torch.tensor(test_input_masks, dtype = torch.long)\n",
    "test_labels = torch.tensor(df_test[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values, dtype = torch.float32)\n",
    "\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_input_ids, train_input_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_input_ids, validation_input_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler = validation_sampler, batch_size = batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_input_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Retrieve the model (In this case the BERTje model without expectations)\n",
    "checkpoint_path = './bert_checkpoints1/'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "final_model_path = os.path.join(checkpoint_path, 'bert_best_model1.pt')\n",
    "model = BertForSequenceClassification.from_pretrained(\"wietsedv/bert-base-dutch-cased\",problem_type=\"multi_label_classification\",  num_labels=3)\n",
    "final_model = torch.load(final_model_path, map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(final_model)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define lists to store the true and predicted labels, and a list to store the wrong predictions\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "wrong_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.cpu()\n",
    "\n",
    "        # Apply sigmoid activation function\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "\n",
    "        # Apply threshold to convert probabilities to binary predictions\n",
    "        threshold = 0.5\n",
    "        predicted_classes = (probabilities > threshold).float()\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "        for idx in range(len(labels)):\n",
    "            true_label = labels[idx].cpu().numpy()\n",
    "            pred_label = predicted_classes[idx].cpu().numpy()\n",
    "            if not all(true_label == pred_label):\n",
    "                review_text = df_test.iloc[i * batch_size + idx][\"reviewText\"]\n",
    "                wrong_pred.append([review_text, true_label, pred_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the wrong predictions\n",
    "df_wrong_pred = pd.DataFrame(wrong_pred, columns=[\"reviewText\", \"trueLabel\", \"predictedlabel\"])\n",
    "\n",
    "# Get the directory of the current script\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "csv_path = os.path.join(script_dir, \"wrong_predictions.csv\")\n",
    "df_wrong_pred.to_csv(csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
