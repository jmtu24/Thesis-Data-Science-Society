{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this notebook was run on an external GPU in a Python file. \n",
    "# Therefore, the code contains code to let it run on a GPU and does it save plots instead of showing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ff64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, auc, roc_curve, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizerFast as BertTokenizer, AdamW\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b85fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link to the dataset used for this thesis\n",
    "# https://www.kaggle.com/datasets/dkapitan/dutch-restaurant-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and transfer it to a dataframe\n",
    "file_path = file_path # Add the path to the file\n",
    "df_raw_data = pd.read_parquet(file_path) # load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of the raw dataset\n",
    "print(df_raw_data.head(5)) # Print the first five rows of the data\n",
    "print(len(df_raw_data)) # Print the number of rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f732042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary preprocessing steps for thesis\n",
    "df_pp_data = df_raw_data.copy()\n",
    "\n",
    "# Remove the unknown characters in the avgPrice column\n",
    "df_pp_data[\"avgPrice\"] = df_pp_data[\"avgPrice\"].str.replace(\"â\\u0082¬\",\"\")\n",
    "df_pp_data[\"avgPrice\"] = df_pp_data[\"avgPrice\"].str.replace(\"\\u0080\",\"\")\n",
    "\n",
    "# Convert the necessary numeric columns to numeric\n",
    "columns_to_convert = [\"scoreTotal\", \"avgPrice\", \"reviewerNumReviews\", \"reviewScoreOverall\", \"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\"]\n",
    "df_pp_data[columns_to_convert] = df_pp_data[columns_to_convert].apply(pd.to_numeric, errors = \"coerce\")\n",
    "\n",
    "# Remove reviews that are not positive and not negative\n",
    "df_pp_data = df_pp_data[~((df_pp_data[\"reviewScoreFood\"] == 5) | (df_pp_data[\"reviewScoreFood\"] == 6) |\n",
    "                         (df_pp_data[\"reviewScoreService\"] == 5) | (df_pp_data[\"reviewScoreService\"] == 6) |\n",
    "                           (df_pp_data[\"reviewScoreAmbiance\"] == 5) | (df_pp_data[\"reviewScoreAmbiance\"] == 6))]\n",
    "\n",
    "# change the personal review scores to sentiments\n",
    "columns_to_change = [\"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\"]\n",
    "bins = [1,6,10]\n",
    "labels = [\"Negative\", \"Positive\"]\n",
    "for column in columns_to_change:\n",
    "    df_pp_data[column] = pd.cut(df_pp_data[column], bins = bins, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ac735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the city name from the address\n",
    "def extract_second_to_last_element(address): # Function to extract the second to last element from the address\n",
    "    address_components = address.split()\n",
    "    if len(address_components) >= 2:\n",
    "        return address_components[-2]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_pp_data[\"City\"] = df_pp_data[\"address\"].apply(extract_second_to_last_element) # create the new feature\n",
    "\n",
    "unique_values_city = df_pp_data[\"City\"].value_counts() # Store the number of unique cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that contain missing values for the important variables\n",
    "# important variables: restoId, avgPrice, reviewScoreOverall, reviewScoreFood, reviewScoreService, reviewScoreAmbiance, reviewText, City\n",
    "df_nomissing = df_pp_data.copy()\n",
    "\n",
    "important_features = [\"restoId\", \"avgPrice\", \"reviewScoreOverall\", \"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\", \"reviewText\", \"City\"]\n",
    "\n",
    "df_nomissing = df_nomissing.dropna(subset = important_features)\n",
    "\n",
    "# Remove reviews that are too short and too long\n",
    "df_nomissing[\"reviewLength\"] = df_nomissing[\"reviewText\"].apply(len) # make a new feature that contains the length of a review\n",
    "\n",
    "descriptives_length = df_nomissing[\"reviewLength\"].describe()\n",
    "\n",
    "df_lessreviews = df_nomissing.copy()\n",
    "df_lessreviews = df_lessreviews[(df_lessreviews[\"reviewLength\"] >= 20) & (df_lessreviews[\"reviewLength\"] <= 2000)] # Reviews with less than 20 characters and more than 2000 characters will be removed\n",
    "\n",
    "# Randomly remove rows with all positive labels to make the dataset more balanced (is still imbalanced after, but less imbalanced),\n",
    "# also remove rows because the amount of data is too computationally expensive\n",
    "\n",
    "sentiment_columns = [\"reviewScoreFood\", \"reviewScoreService\", \"reviewScoreAmbiance\"]\n",
    "df_sentiment = df_lessreviews[sentiment_columns]\n",
    "\n",
    "positive_rows = np.all(df_sentiment == \"Positive\", axis = 1) # Define which reviews only contain positive sentiments\n",
    "\n",
    "sampled_positive_rows = df_lessreviews[positive_rows].sample(n = 232830, random_state = 68)\n",
    "\n",
    "df_lessreviews = df_lessreviews.drop(sampled_positive_rows.index)\n",
    "\n",
    "# analyze how many positive and negative reviews are left in the reduced dataframe\n",
    "counts_food = df_lessreviews[\"reviewScoreFood\"].value_counts()\n",
    "counts_service = df_lessreviews[\"reviewScoreService\"].value_counts()\n",
    "counts_ambiance = df_lessreviews[\"reviewScoreAmbiance\"].value_counts()\n",
    "\n",
    "print(counts_food)\n",
    "print(counts_service)\n",
    "print(counts_ambiance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9477e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentiments into labels (negative = 0, positive = 1)\n",
    "class_mapping = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "# Initialize LabelEncoder with the custom mapping\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = class_mapping.keys()\n",
    "label_encoder.transform = lambda x: [class_mapping[label] for label in x]\n",
    "\n",
    "# Fit and transform the target variables using the custom mapping\n",
    "df_lessreviews[\"labelFood\"] = label_encoder.transform(df_lessreviews[\"reviewScoreFood\"])\n",
    "df_lessreviews[\"labelService\"] = label_encoder.transform(df_lessreviews[\"reviewScoreService\"])\n",
    "df_lessreviews[\"labelAmbiance\"] = label_encoder.transform(df_lessreviews[\"reviewScoreAmbiance\"])\n",
    "df_lessreviews.head()\n",
    "\n",
    "# Normalize the price variable\n",
    "scaler = MinMaxScaler()\n",
    "df_lessreviews[\"avgPrice\"] = scaler.fit_transform(df_lessreviews[[\"avgPrice\"]])\n",
    "\n",
    "# Embed the city variable\n",
    "label_encoder_city = LabelEncoder()\n",
    "df_lessreviews[\"city_encoded\"] = label_encoder_city.fit_transform(df_lessreviews[\"City\"])\n",
    "amount_cities = len(label_encoder_city.classes_) # extract the number of unique locations\n",
    "embedding_dim_city = int(amount_cities**0.5) # use square root rule to define the number of embedding dimensions\n",
    "embedding_city = nn.Embedding(amount_cities, embedding_dim_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a train, validation, and test set\n",
    "df_final = df_lessreviews\n",
    "\n",
    "# First split the data into train and temp sets\n",
    "df_train, df_temp = train_test_split(df_final, test_size= 0.2, random_state=68)\n",
    "\n",
    "# Then split the temp set into validation and test sets\n",
    "df_validation, df_test = train_test_split(df_temp, test_size= 0.5, random_state=68)\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print(\"Training set samples:\", len(df_train))\n",
    "print(\"Validation set samples:\", len(df_validation))\n",
    "print(\"Test set samples:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbaeda",
   "metadata": {},
   "source": [
    "# Optimize and run BERTje without expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34faad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data for BERTje\n",
    "tokenizer_BERTje = BertTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\") # load the tokenizer for BERTje\n",
    "\n",
    "# The following code for tokenizing is based on the function wrap_examples(examples, tokenizer) available here: https://github.com/wietsedv/bertje/blob/master/finetuning/v1/run_110kDBRD.py\n",
    "# I used parts of this function to tokenize the reviews and return them as a dataset for my train, validation, and test samples seperately.\n",
    "\n",
    "# Tokenize the reviews in the training set\n",
    "train_input_ids, train_input_masks, train_labels = [], [], []\n",
    "for text in df_train[\"reviewText\"]:\n",
    "    tokenized_bert_train = tokenizer_BERTje.encode_plus(text, max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    train_input_ids.append(tokenized_bert_train[\"input_ids\"])\n",
    "    train_input_masks.append(tokenized_bert_train[\"attention_mask\"])\n",
    "\n",
    "train_input_ids = torch.tensor(train_input_ids, dtype = torch.long)\n",
    "train_input_masks = torch.tensor(train_input_masks, dtype = torch.long)\n",
    "train_labels = torch.tensor(df_train[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values, dtype = torch.float32)\n",
    "\n",
    "# Tokenize the reviews in the validation set\n",
    "validation_input_ids, validation_input_masks, validation_labels = [], [], []\n",
    "for text in df_validation[\"reviewText\"]:\n",
    "    tokenized_bert_validation = tokenizer_BERTje.encode_plus(text, max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    validation_input_ids.append(tokenized_bert_validation[\"input_ids\"])\n",
    "    validation_input_masks.append(tokenized_bert_validation[\"attention_mask\"])\n",
    "\n",
    "validation_input_ids = torch.tensor(validation_input_ids, dtype = torch.long)\n",
    "validation_input_masks = torch.tensor(validation_input_masks, dtype = torch.long)\n",
    "validation_labels = torch.tensor(df_validation[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values, dtype = torch.float32)\n",
    "\n",
    "# Tokenize the reviews in the test set\n",
    "test_input_ids, test_input_masks, test_labels = [], [], []\n",
    "for text in df_test[\"reviewText\"]:\n",
    "    tokenized_bert_test = tokenizer_BERTje.encode_plus(text, max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    test_input_ids.append(tokenized_bert_test[\"input_ids\"])\n",
    "    test_input_masks.append(tokenized_bert_test[\"attention_mask\"])\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids, dtype = torch.long)\n",
    "test_input_masks = torch.tensor(test_input_masks, dtype = torch.long)\n",
    "test_labels = torch.tensor(df_test[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values, dtype = torch.float32)\n",
    "\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_input_ids, train_input_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_input_ids, validation_input_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler = validation_sampler, batch_size = batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_input_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94800990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to optimize the parameters for the BERTje model without expectations\n",
    "\n",
    "# This function is based on the function train_model(model, device, train_dataloader, dev_dataloader, output_path, num_epochs=4, lr=2e-5, eps=1e-8, max_grad_norm=1.0, seed=4327)\n",
    "# Retrieved from: https://github.com/wietsedv/bertje/blob/master/finetuning/v1/run_110kDBRD.py\n",
    "# The function from Wietse et al. (2019) only takes one set of parameters and my function optimizes a set of parameters, also my function has an early stopping criterion\n",
    "\n",
    "def train_and_evaluate(lr, batch_size, epochs, train_dataset, val_dataset, patience = 1):\n",
    "    # Initialize DataLoader with the given batch size\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = BertForSequenceClassification.from_pretrained(\"wietsedv/bert-base-dutch-cased\", problem_type=\"multi_label_classification\", num_labels=3)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Make lists to store train and validation losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0 # reset training loss\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0 # reset validation loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_dataloader:\n",
    "                val_input_ids, val_attention_mask, val_labels = val_batch\n",
    "                val_input_ids, val_attention_mask, val_labels = val_input_ids.to(device), val_attention_mask.to(device), val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
    "                val_logits = val_outputs.logits\n",
    "\n",
    "                val_loss += loss_fn(val_logits, val_labels).item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Insert early stopping to prevent overfitting\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return best_val_loss, model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "learning_rates = [1e-5, 2e-5, 3e-5, 4e-5, 5e-5]\n",
    "batch_sizes = [8, 16, 32]\n",
    "epochs = 10\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_val_loss = float('inf')\n",
    "best_hyperparams = None\n",
    "best_model = None\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "# Make dictionaries to store train and validation losses\n",
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "\n",
    "# Train and validate the BERTje model for each set of the defined hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Training with lr={lr}, batch_size={batch_size}\")\n",
    "        val_loss, model, train_losses, val_losses = train_and_evaluate(lr, batch_size, epochs, train_data, validation_data)\n",
    "        \n",
    "        key = f\"lr_{lr}_bs_{batch_size}\"\n",
    "        all_train_losses[key] = train_losses\n",
    "        all_val_losses[key] = val_losses\n",
    "        \n",
    "        # Define and store the best hyperparameters and model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_hyperparams = (lr, batch_size)\n",
    "            best_model = model\n",
    "\n",
    "\n",
    "# Define where the best model should be saved\n",
    "checkpoint_path = './bert_checkpoints1/'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "# save the best model\n",
    "final_model_path = os.path.join(checkpoint_path, 'bert_best_model1.pt')\n",
    "torch.save(best_model.state_dict(), final_model_path)\n",
    "\n",
    "\n",
    "print(f\"Best hyperparameters: Learning rate = {best_hyperparams[0]}, Batch size = {best_hyperparams[1]}\")\n",
    "print(f\"Best model saved at: {final_model_path}\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee4f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss for the best model\n",
    "# Retrieve the best hyperparameters\n",
    "best_lr, best_batch_size = best_hyperparams\n",
    "best_key = f\"lr_{best_lr}_bs_{best_batch_size}\"\n",
    "\n",
    "# Retrieve the data for the best hyperparameters\n",
    "best_train_losses = all_train_losses[best_key]\n",
    "best_val_losses = all_val_losses[best_key]\n",
    "\n",
    "# Plot Losses for the best model\n",
    "min_length = min(len(best_train_losses), len(best_val_losses))\n",
    "epochs_range = range(1, min_length + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_range, best_train_losses[:min_length], label='Train Loss', marker='o')\n",
    "plt.plot(epochs_range, best_val_losses[:min_length], label='Val Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training and Validation Losses for BERTje')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the loss plot\n",
    "plt.savefig(f'training_validation_losses_BERTje.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89aee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimal model and retrieve evaluation metrics and plots\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the final model for evaluation\n",
    "model = BertForSequenceClassification.from_pretrained(\"wietsedv/bert-base-dutch-cased\",problem_type=\"multi_label_classification\",  num_labels=3)\n",
    "model.load_state_dict(torch.load(final_model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define lists to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Make predictions with the best model\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.cpu()\n",
    "\n",
    "        # Apply sigmoid activation function\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "\n",
    "        # Apply threshold to convert probabilities to binary predictions\n",
    "        threshold = 0.5\n",
    "        predicted_classes = (probabilities > threshold).float()\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "# convert the true and predicted labels to numpy arrays\n",
    "true_labels_array = np.array(true_labels)\n",
    "pred_labels_array = np.array(pred_labels)\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "# Compute precision, recall, and F1-score for each label\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(true_labels, pred_labels, average=None)\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "label_names = [\"labelFood\", \"labelService\", \"labelAmbiance\"]\n",
    "class_labels = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Print detailed metrics for each label\n",
    "for i, label in enumerate(label_names):\n",
    "    print(f\"{label}:\")\n",
    "    print(f\"  Precision: {precision[i]}\")\n",
    "    print(f\"  Recall: {recall[i]}\")\n",
    "    print(f\"  F1-score: {fscore[i]}\")\n",
    "    print(f\"  Support: {support[i]}\")\n",
    "\n",
    "\n",
    "# print the classification report per label and make confusion matrices per label\n",
    "for label_index, label_name in enumerate(label_names):\n",
    "    # Binarize the true and predicted labels for the current label\n",
    "    binary_true_labels = true_labels_array[:, label_index]\n",
    "    binary_pred_labels = pred_labels_array[:, label_index]\n",
    "    label_accuracy = accuracy_score(binary_true_labels, binary_pred_labels)\n",
    "\n",
    "    # Print the classification report and accuracy for the current label\n",
    "    print(f\"Classification Report for {label_name}:\")\n",
    "    print(classification_report(binary_true_labels, binary_pred_labels, target_names=[\"Negative\", \"Positive\"], labels = [0, 1]))\n",
    "    print(f\"Accuracy: {label_accuracy}\\n\")\n",
    "\n",
    "    cm = confusion_matrix(binary_true_labels, binary_pred_labels)\n",
    "\n",
    "    # Plot the confusion matrix for the current label\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels= [\"Negative\", \"Positive\"], yticklabels= [\"Negative\", \"Positive\"])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f'Confusion Matrix - BERTje {label_name}')\n",
    "    plt.savefig(f'confusion_matrix_{label_name}.png') # Save the plot\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5128b",
   "metadata": {},
   "source": [
    "# Optimize and run BERTje with expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16526d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data for BERTje with expectations\n",
    "tokenizer_BERTje = BertTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\") # load the tokenizer for BERTje\n",
    "\n",
    "# The following code for tokenizing is based on the function wrap_examples(examples, tokenizer) available here: https://github.com/wietsedv/bertje/blob/master/finetuning/v1/run_110kDBRD.py\n",
    "# I used parts of this function to tokenize the reviews and return them as a dataset for my train, validation, and test samples seperately.\n",
    "\n",
    "# Tokenize the reviews and convert the city encodings to PyTorch tensors in the training set\n",
    "train_input_ids, train_input_masks, train_labels, train_cities, train_prices = [], [], [], [], []\n",
    "for i, row in df_train.iterrows():\n",
    "    tokenized_bert_train = tokenizer_BERTje.encode_plus(row[\"reviewText\"], max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    train_input_ids.append(tokenized_bert_train[\"input_ids\"])\n",
    "    train_input_masks.append(tokenized_bert_train[\"attention_mask\"])\n",
    "    train_labels.append(row[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values)\n",
    "    train_cities.append(embedding_city(torch.LongTensor([row[\"city_encoded\"]])).squeeze())\n",
    "    train_prices.append(row[\"avgPrice\"])\n",
    "\n",
    "train_input_ids = torch.tensor(train_input_ids, dtype = torch.long)\n",
    "train_input_masks = torch.tensor(train_input_masks, dtype = torch.long)\n",
    "train_labels = torch.tensor(train_labels, dtype = torch.float32)\n",
    "train_cities = torch.stack(train_cities)\n",
    "train_prices = torch.tensor(train_prices, dtype=torch.float32)\n",
    "\n",
    "# Tokenize the reviews and convert the city encodings to PyTorch tensors in the validation set\n",
    "validation_input_ids, validation_input_masks, validation_labels, validation_cities, validation_prices = [], [], [], [], []\n",
    "for i, row in df_validation.iterrows():\n",
    "    tokenized_bert_validation = tokenizer_BERTje.encode_plus(row[\"reviewText\"], max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    validation_input_ids.append(tokenized_bert_validation[\"input_ids\"])\n",
    "    validation_input_masks.append(tokenized_bert_validation[\"attention_mask\"])\n",
    "    validation_labels.append(row[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values)\n",
    "    validation_cities.append(embedding_city(torch.LongTensor([row[\"city_encoded\"]])).squeeze())\n",
    "    validation_prices.append(row[\"avgPrice\"])\n",
    "\n",
    "validation_input_ids = torch.tensor(validation_input_ids, dtype = torch.long)\n",
    "validation_input_masks = torch.tensor(validation_input_masks, dtype = torch.long)\n",
    "validation_labels = torch.tensor(validation_labels, dtype = torch.float32)\n",
    "validation_cities = torch.stack(validation_cities)\n",
    "validation_prices = torch.tensor(validation_prices, dtype=torch.float32)\n",
    "\n",
    "# Tokenize the reviews in the test set\n",
    "test_input_ids, test_input_masks, test_labels, test_cities, test_prices = [], [], [], [], []\n",
    "for i, row in df_test.iterrows():\n",
    "    tokenized_bert_test = tokenizer_BERTje.encode_plus(row[\"reviewText\"], max_length = 512, truncation = True, add_special_tokens = True,\n",
    "                                                padding = \"max_length\", return_token_type_ids = False)\n",
    "    test_input_ids.append(tokenized_bert_test[\"input_ids\"])\n",
    "    test_input_masks.append(tokenized_bert_test[\"attention_mask\"])\n",
    "    test_labels.append(row[[\"labelFood\", \"labelService\", \"labelAmbiance\"]].values)\n",
    "    test_cities.append(embedding_city(torch.LongTensor([row[\"city_encoded\"]])).squeeze())\n",
    "    test_prices.append(row[\"avgPrice\"])\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids, dtype = torch.long)\n",
    "test_input_masks = torch.tensor(test_input_masks, dtype = torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype = torch.float32)\n",
    "test_cities = torch.stack(test_cities)\n",
    "test_prices = torch.tensor(test_prices, dtype=torch.float32)\n",
    "\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_input_ids, train_input_masks, train_labels, train_cities, train_prices)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_input_ids, validation_input_masks, validation_labels, validation_cities, validation_prices)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler = validation_sampler, batch_size = batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_input_masks, test_labels, test_cities, test_prices)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34520175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to optimize the parameters for the BERTje model with expectations\n",
    "\n",
    "# This function is based on the function train_model(model, device, train_dataloader, dev_dataloader, output_path, num_epochs=4, lr=2e-5, eps=1e-8, max_grad_norm=1.0, seed=4327)\n",
    "# Retrieved from: https://github.com/wietsedv/bertje/blob/master/finetuning/v1/run_110kDBRD.py\n",
    "# The function from Wietse et al. (2019) only takes one set of parameters and my function optimizes a set of parameters, also my function inserts an early stopping criterion\n",
    "\n",
    "def train_and_evaluate(lr, batch_size, epochs, train_dataset, val_dataset, patience = 1):\n",
    "    # Initialize DataLoader with the given batch size\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = BertForSequenceClassification.from_pretrained(\"wietsedv/bert-base-dutch-cased\", problem_type=\"multi_label_classification\", num_labels=3)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Make lists to store train and validation losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0 # reset training loss\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids, attention_mask, labels, cities, prices = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels, cities, prices = input_ids.to(device), attention_mask.to(device), labels.to(device), cities.to(device), prices.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0 # reset validation loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_dataloader:\n",
    "                val_input_ids, val_attention_mask, val_labels, val_cities, val_prices = val_batch\n",
    "                val_input_ids, val_attention_mask, val_labels, val_cities, val_prices = val_input_ids.to(device), val_attention_mask.to(device), val_labels.to(device), val_cities.to(device), val_prices.to(device)\n",
    "\n",
    "                val_outputs = model(val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
    "                val_logits = val_outputs.logits\n",
    "\n",
    "                val_loss += loss_fn(val_logits, val_labels).item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Insert early stopping to prevent overfitting\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return best_val_loss, model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e23491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "learning_rates = [1e-5, 2e-5, 3e-5, 4e-5, 5e-5]\n",
    "batch_sizes = [8, 16, 32]\n",
    "epochs = 10\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_val_loss = float('inf')\n",
    "best_hyperparams = None\n",
    "best_model = None\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "# Make dictionaries to store train and validation losses\n",
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "\n",
    "# Train and validate the BERTje model for each set of the defined hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Training with lr={lr}, batch_size={batch_size}\")\n",
    "        val_loss, model, train_losses, val_losses = train_and_evaluate(lr, batch_size, epochs, train_data, validation_data)\n",
    "        \n",
    "        key = f\"lr_{lr}_bs_{batch_size}\"\n",
    "        all_train_losses[key] = train_losses\n",
    "        all_val_losses[key] = val_losses\n",
    "        \n",
    "        # Define and store the best hyperparameters and model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_hyperparams = (lr, batch_size)\n",
    "            best_model = model\n",
    "\n",
    "\n",
    "# Define where the best model should be saved\n",
    "checkpoint_path = './bert_checkpoints2/'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "# save the best model\n",
    "final_model_path = os.path.join(checkpoint_path, 'bert_best_model2.pt')\n",
    "torch.save(best_model.state_dict(), final_model_path)\n",
    "\n",
    "\n",
    "print(f\"Best hyperparameters: Learning rate = {best_hyperparams[0]}, Batch size = {best_hyperparams[1]}\")\n",
    "print(f\"Best model saved at: {final_model_path}\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss for the best model\n",
    "# Retrieve the best hyperparameters\n",
    "best_lr, best_batch_size = best_hyperparams\n",
    "best_key = f\"lr_{best_lr}_bs_{best_batch_size}\"\n",
    "\n",
    "# Retrieve the data for the best hyperparameters\n",
    "best_train_losses = all_train_losses[best_key]\n",
    "best_val_losses = all_val_losses[best_key]\n",
    "\n",
    "# Plot Losses for the best model\n",
    "min_length = min(len(best_train_losses), len(best_val_losses))\n",
    "epochs_range = range(1, min_length + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_range, best_train_losses[:min_length], label='Train Loss', marker='o')\n",
    "plt.plot(epochs_range, best_val_losses[:min_length], label='Val Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training and Validation Losses for BERTje with expectations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the loss plot\n",
    "plt.savefig(f'training_validation_losses_BERTje_expectations.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa02ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimal model and retrieve evaluation metrics and plots\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the final model for evaluation\n",
    "model = BertForSequenceClassification.from_pretrained(\"wietsedv/bert-base-dutch-cased\",problem_type=\"multi_label_classification\",  num_labels=3)\n",
    "model.load_state_dict(torch.load(final_model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define lists to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Make predictions with the best model\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, labels, cities, prices = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.cpu()\n",
    "\n",
    "        # Apply sigmoid activation function\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "\n",
    "        # Apply threshold to convert probabilities to binary predictions\n",
    "        threshold = 0.5\n",
    "        predicted_classes = (probabilities > threshold).float()\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "# convert the true and predicted labels to numpy arrays\n",
    "true_labels_array = np.array(true_labels)\n",
    "pred_labels_array = np.array(pred_labels)\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "# Compute precision, recall, and F1-score for each label\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(true_labels, pred_labels, average=None)\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "label_names = [\"labelFood\", \"labelService\", \"labelAmbiance\"]\n",
    "class_labels = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Print detailed metrics for each label\n",
    "for i, label in enumerate(label_names):\n",
    "    print(f\"{label}:\")\n",
    "    print(f\"  Precision: {precision[i]}\")\n",
    "    print(f\"  Recall: {recall[i]}\")\n",
    "    print(f\"  F1-score: {fscore[i]}\")\n",
    "    print(f\"  Support: {support[i]}\")\n",
    "\n",
    "\n",
    "# print the classification report per label and make confusion matrices per label\n",
    "for label_index, label_name in enumerate(label_names):\n",
    "    # Binarize the true and predicted labels for the current label\n",
    "    binary_true_labels = true_labels_array[:, label_index]\n",
    "    binary_pred_labels = pred_labels_array[:, label_index]\n",
    "    label_accuracy = accuracy_score(binary_true_labels, binary_pred_labels)\n",
    "\n",
    "    # Print the classification report and accuracy for the current label\n",
    "    print(f\"Classification Report for {label_name} with expectations:\")\n",
    "    print(classification_report(binary_true_labels, binary_pred_labels, target_names=[\"Negative\", \"Positive\"], labels = [0, 1]))\n",
    "    print(f\"Accuracy: {label_accuracy}\\n\")\n",
    "\n",
    "    cm = confusion_matrix(binary_true_labels, binary_pred_labels)\n",
    "\n",
    "    # Plot the confusion matrix for the current label\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels= [\"Negative\", \"Positive\"], yticklabels= [\"Negative\", \"Positive\"])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f'Confusion Matrix - BERTje with expectations {label_name}')\n",
    "    plt.savefig(f'confusion_matrix_{label_name}_expectations.png') # Save the plot\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
